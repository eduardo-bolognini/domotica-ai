{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0298cb5a",
   "metadata": {},
   "source": [
    "## Clustering: Explanation\n",
    "The objective is to create a structured dataset to train the model. The process goes through three phases of hierarchical clustering that build an increasingly complex representation: first, individual people are isolated and categorized, then these poses are combined to represent the overall situation of the image, and finally the temporal dimension and sensory context are added to capture behavioral routines. This progression from particular to general allows building a reusable vocabulary at each level, reducing computational complexity and improving generalization of the final models.\n",
    "\n",
    "For person detection, two versions of YOLO are used: a standard one and one specialized for grayscale (for night mode). The grayscale model can be trained following the guide available at https://docs.ultralytics.com/it/datasets/detect/coco8-grayscale/#usage. CLIP extracts semantic features from detected people, while K-means groups similar elements (very often centroids are also leveraged to order clusters by similarity).\n",
    "\n",
    "The result is a tripartite dataset: vocabulary of base poses, library of activities as meaningful combinations of poses, and contextualized routines with sensory correlations. This hierarchical structure enables training models that understand human behavior at multiple temporal scales, from instantaneous poses to complete daily routines.\n",
    "\n",
    "Use \"reviewer.js\" to revise clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b22ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==2.2.6 pandas==2.3.0 scikit-learn==1.7.0 pillow torch==2.3.0 torchvision==0.18.0 open-clip-torch==2.32.0 ultralytics==8.3.165 tqdm opencv-python==4.11.0.86\n",
    "\n",
    "# PYTHON VERSION USED: 3.10.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ad13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import shutil \n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "import cv2\n",
    "from collections import defaultdict, deque\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import gc\n",
    "import time\n",
    "\n",
    "import open_clip\n",
    "from ultralytics import YOLO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "# import timm -> for ViT (not used) -> pip install timm\n",
    "\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88159066",
   "metadata": {},
   "source": [
    "### Definizione di costanti, modello e funzioni di utilità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cfb711",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"Home Assistant Monitor Complete Jul 12 2025\"\n",
    "PATH_IMAGES = os.path.join(BASE_DIR, \"images\")\n",
    "PATH_CSV = os.path.join(BASE_DIR, \"stati_home_assistant.csv\")\n",
    "\n",
    "yolo_model = YOLO(\"yolo12l.pt\") \n",
    "yolo_model = yolo_model.to(device).eval()\n",
    "\n",
    "yolo_model_grayscale = YOLO('yolo12l-grayscale.pt')\n",
    "yolo_model_grayscale = yolo_model_grayscale.to(device).eval() # to load on MPS install torch==2.3.0 torchvision==0.18.0 (just indicated in requirements)\n",
    "\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "clip_model = clip_model.to(device).eval()\n",
    "\n",
    "# ViT - not used in this version\n",
    "# vit = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=0)\n",
    "# vit = vit.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cb1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_bbox(bbox, img_w, img_h, expansion_ratio=0.3):\n",
    "    xmin, ymin, xmax, ymax = bbox\n",
    "    w, h = xmax - xmin, ymax - ymin\n",
    "    pad_w, pad_h = w * expansion_ratio, h * expansion_ratio\n",
    "    xmin_e = max(int(xmin - pad_w), 0)\n",
    "    ymin_e = max(int(ymin - pad_h), 0)\n",
    "    xmax_e = min(int(xmax + pad_w), img_w - 1)\n",
    "    ymax_e = min(int(ymax + pad_h), img_h - 1)\n",
    "    return xmin_e, ymin_e, xmax_e, ymax_e\n",
    "\n",
    "\n",
    "def detect_grayscale(image):\n",
    "    if image.mode == 'L':\n",
    "        return True\n",
    "    elif image.mode == 'RGB':\n",
    "        img_array = np.array(image)\n",
    "        r, g, b = img_array[:,:,0], img_array[:,:,1], img_array[:,:,2]\n",
    "        return np.array_equal(r, g) and np.array_equal(g, b)\n",
    "    return False\n",
    "\n",
    "def enhance_grayscale(image):\n",
    "    if image.mode != 'L':\n",
    "        image = image.convert('L')\n",
    "    \n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    image = enhancer.enhance(1.4)\n",
    "    \n",
    "    img_array = np.array(image)\n",
    "    img_array = cv2.equalizeHist(img_array)\n",
    "    image = Image.fromarray(img_array)\n",
    "    \n",
    "    return Image.merge('RGB', (image, image, image))\n",
    "\n",
    "def extract_clip_embedding(image, is_grayscale=False):\n",
    "    if is_grayscale:\n",
    "        embeddings = []\n",
    "        \n",
    "        img_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            feat = clip_model.encode_image(img_tensor)\n",
    "            feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "        embeddings.append(feat.squeeze().cpu().numpy())\n",
    "        \n",
    "        enhancer = ImageEnhance.Contrast(image)\n",
    "        img_enhanced = enhancer.enhance(1.2)\n",
    "        img_tensor = preprocess(img_enhanced).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            feat = clip_model.encode_image(img_tensor)\n",
    "            feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "        embeddings.append(feat.squeeze().cpu().numpy())\n",
    "        \n",
    "        final_embedding = np.mean(embeddings, axis=0)\n",
    "        final_embedding = final_embedding / np.linalg.norm(final_embedding)\n",
    "        return final_embedding\n",
    "    else:\n",
    "        img_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            feat = clip_model.encode_image(img_tensor)\n",
    "            feat = feat / feat.norm(dim=-1, keepdim=True)\n",
    "        return feat.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union_area = area1 + area2 - inter_area\n",
    "    return inter_area / union_area if union_area > 0 else 0.0\n",
    "\n",
    "def group_overlapping_bboxes(bboxes, threshold=0.5):\n",
    "    n = len(bboxes)\n",
    "    graph = defaultdict(list)\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if calculate_iou(bboxes[i][:4], bboxes[j][:4]) >= threshold:\n",
    "                graph[i].append(j)\n",
    "                graph[j].append(i)\n",
    "    visited = [False] * n\n",
    "    groups = []\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            queue = deque([i])\n",
    "            group = []\n",
    "            while queue:\n",
    "                node = queue.popleft()\n",
    "                if visited[node]:\n",
    "                    continue\n",
    "                visited[node] = True\n",
    "                group.append(bboxes[node])\n",
    "                queue.extend(graph[node])\n",
    "            if len(group) > 1:\n",
    "                groups.append(group)\n",
    "    return groups\n",
    "\n",
    "def merge_overlapping_boxes(bboxes, threshold=0.5):\n",
    "    overlapping_groups = group_overlapping_bboxes(bboxes, threshold)\n",
    "    merged = []\n",
    "\n",
    "    used = set()\n",
    "    for group in overlapping_groups:\n",
    "        x1 = min(b[0] for b in group)\n",
    "        y1 = min(b[1] for b in group)\n",
    "        x2 = max(b[2] for b in group)\n",
    "        y2 = max(b[3] for b in group)\n",
    "        score = np.mean([b[4] for b in group])\n",
    "        merged.append([x1, y1, x2, y2, score])\n",
    "        used.update(tuple(b) for b in group)\n",
    "\n",
    "    for b in bboxes:\n",
    "        if tuple(b) not in used:\n",
    "            merged.append(b)\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def extract_scene_embedding(image_path):\n",
    "    pil_img = Image.open(image_path).convert(\"RGB\")\n",
    "    pil_img = pil_img.resize((224, 224), Image.LANCZOS)\n",
    "    img_tensor = torch.from_numpy(np.array(pil_img)).permute(2, 0, 1).float() / 255.0\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = vit(img_tensor)  # 768 dimensioni\n",
    "    \n",
    "    return features.squeeze().cpu().numpy()\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24691bfc",
   "metadata": {},
   "source": [
    "### STEP 1: Image Processing and Activity Clustering\n",
    "Instead of treating each person as a unique entity, reusable activity categories (poses) are created. This drastically reduces complexity: from thousands of different people to a few dozen activity-types (sitting at desk with computer, standing while tidying the room, lying in bed with phone).\n",
    "Manual review with revisor.js serves to clean up the automated results. The clusters_kmeans_1000 folder is configured by disabling group mode and annotations. The objective is to obtain semantically coherent poses that will become the building blocks of subsequent steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b588a305",
   "metadata": {},
   "source": [
    "#### Image processing and reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3425916",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_info = []\n",
    "for root, _, files in os.walk(PATH_IMAGES):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            try:\n",
    "                ts = os.path.splitext(file)[0]\n",
    "                dt = datetime.strptime(ts, \"%d-%m_%H-%M-%S\")\n",
    "                image_info.append((dt, os.path.join(root, file)))\n",
    "            except ValueError:\n",
    "                continue\n",
    "image_info.sort()\n",
    "\n",
    "all_image_paths = [p for _, p in image_info]\n",
    "\n",
    "print(len(all_image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef51cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_quadrants_metadata(w, h, overlap_ratio=0.10):\n",
    "    overlap_x = int(w * overlap_ratio)\n",
    "    overlap_y = int(h * overlap_ratio)\n",
    "    half_w = w // 2\n",
    "    half_h = h // 2\n",
    "\n",
    "    quadrants = [\n",
    "        (\"q1\", (0, 0, half_w + overlap_x, half_h + overlap_y), (0, 0)),\n",
    "        (\"q2\", (half_w - overlap_x, 0, w, half_h + overlap_y), (half_w - overlap_x, 0)),\n",
    "        (\"q3\", (0, half_h - overlap_y, half_w + overlap_x, h), (0, half_h - overlap_y)),\n",
    "        (\"q4\", (half_w - overlap_x, half_h - overlap_y, w, h), (half_w - overlap_x, half_h - overlap_y)),\n",
    "    ]\n",
    "    \n",
    "    corrected_quadrants = []\n",
    "    for quad_name, crop_coords, offset in quadrants:\n",
    "        crop_coords = (\n",
    "            max(0, crop_coords[0]),\n",
    "            max(0, crop_coords[1]), \n",
    "            min(w, crop_coords[2]),\n",
    "            min(h, crop_coords[3])\n",
    "        )\n",
    "        corrected_quadrants.append((quad_name, crop_coords, offset))\n",
    "    \n",
    "    return corrected_quadrants\n",
    "\n",
    "def prepare_quadrants(pil_img, overlap_ratio=0.10):\n",
    "    w, h = pil_img.size\n",
    "    return prepare_quadrants_metadata(w, h, overlap_ratio)\n",
    "\n",
    "def process_quadrants_batch(metadata_list, model, device, is_grayscale_batch=False):\n",
    "    all_results = []\n",
    "    \n",
    "    conf_threshold = 0.4 if is_grayscale_batch else 0.5\n",
    "    \n",
    "    quad_images = []\n",
    "    quad_metadata = []\n",
    "    \n",
    "    for img_metadata in metadata_list:\n",
    "        img_path = img_metadata['path']\n",
    "        quadrants = img_metadata['quadrants']\n",
    "        \n",
    "        pil_img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        for _, crop_coords, offset in quadrants:\n",
    "            quad_img = pil_img.crop(crop_coords)\n",
    "            quad_images.append(quad_img)\n",
    "            quad_metadata.append((img_path, offset, conf_threshold))\n",
    "    \n",
    "    if quad_images:\n",
    "        batch_size = 16\n",
    "        \n",
    "        batch_results = []\n",
    "        for i in range(0, len(quad_images), batch_size):\n",
    "            batch_imgs = quad_images[i:i+batch_size]\n",
    "            with torch.no_grad():\n",
    "                results = model(batch_imgs, verbose=False)\n",
    "                batch_results.extend(results)\n",
    "        \n",
    "        for result, (img_path, offset, conf_threshold) in zip(batch_results, quad_metadata):\n",
    "            img_detections = []\n",
    "            for bbox, cls, conf in zip(result.boxes.xyxy.cpu().numpy(), \n",
    "                                     result.boxes.cls.cpu().numpy(), \n",
    "                                     result.boxes.conf.cpu().numpy()):\n",
    "                if int(cls) == 0 and conf >= conf_threshold:\n",
    "                    xmin, ymin, xmax, ymax = bbox\n",
    "                    xmin += offset[0]\n",
    "                    xmax += offset[0]\n",
    "                    ymin += offset[1]\n",
    "                    ymax += offset[1]\n",
    "                    img_detections.append([xmin, ymin, xmax, ymax, conf])\n",
    "            \n",
    "            all_results.append((img_path, img_detections))\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def extract_embeddings_parallel(detection_data, max_workers=4, expansion=0.4):    \n",
    "    def process_single_detection(args):\n",
    "        img_path, pil_img, bbox, conf, is_grayscale = args\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        \n",
    "        if xmax <= xmin or ymax <= ymin:\n",
    "            return None\n",
    "            \n",
    "        crop = pil_img.crop((\n",
    "            max(0, xmin - expansion * (xmax - xmin)),\n",
    "            max(0, ymin - expansion * (ymax - ymin)),\n",
    "            min(pil_img.width, xmax + expansion * (xmax - xmin)),\n",
    "            min(pil_img.height, ymax + expansion * (ymax - ymin))\n",
    "        ))\n",
    "\n",
    "        if crop.width == 0 or crop.height == 0:\n",
    "            return None\n",
    "\n",
    "        emb = extract_clip_embedding(crop, is_grayscale)\n",
    "        \n",
    "        return {\n",
    "            \"bbox\": [xmin, ymin, xmax, ymax],\n",
    "            \"embedding\": emb,\n",
    "            \"is_grayscale\": is_grayscale,\n",
    "            \"confidence\": conf\n",
    "        }\n",
    "    \n",
    "    results = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_path = {}\n",
    "        \n",
    "        for img_path, detection_list in detection_data.items():\n",
    "            for detection_args in detection_list:\n",
    "                future = executor.submit(process_single_detection, detection_args)\n",
    "                if img_path not in future_to_path:\n",
    "                    future_to_path[img_path] = []\n",
    "                future_to_path[img_path].append(future)\n",
    "        \n",
    "        for img_path, futures in future_to_path.items():\n",
    "            results[img_path] = []\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    results[img_path].append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def process_images_optimized(batch_size_normal=8, batch_size_grayscale=16, checkpoint_every=1000, checkpoint_file=\"checkpoint.pkl\", expansion=0.4):\n",
    "\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        print(f\"Caricamento checkpoint da {checkpoint_file}...\")\n",
    "        with open(checkpoint_file, 'rb') as f:\n",
    "            checkpoint_data = pickle.load(f)\n",
    "        processed_paths = set(checkpoint_data.get('processed_paths', []))\n",
    "        final_results = checkpoint_data.get('results', {})\n",
    "    else:\n",
    "        processed_paths = set()\n",
    "        final_results = {}\n",
    "    \n",
    "    remaining_paths = [path for path in all_image_paths if path not in processed_paths]\n",
    "    \n",
    "    if not remaining_paths:\n",
    "        return final_results\n",
    "    \n",
    "    chunk_size = 5000  \n",
    "    \n",
    "    for chunk_start in range(0, len(remaining_paths), chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, len(remaining_paths))\n",
    "        chunk_paths = remaining_paths[chunk_start:chunk_end]\n",
    "        \n",
    "        print(f\"Processando chunk {chunk_start//chunk_size + 1}/{(len(remaining_paths)-1)//chunk_size + 1}\")\n",
    "        \n",
    "        chunk_grayscale = []\n",
    "        chunk_normal = []\n",
    "        \n",
    "        for img_path in tqdm(chunk_paths, desc=f\"Preprocessing chunk {chunk_start//chunk_size + 1}\"):\n",
    "            try:\n",
    "                with Image.open(img_path) as pil_img:\n",
    "                    pil_img = pil_img.convert(\"RGB\")\n",
    "                    is_grayscale = detect_grayscale(pil_img)\n",
    "                    \n",
    "                    w, h = pil_img.size\n",
    "                    quadrant_metadata = prepare_quadrants_metadata(w, h)\n",
    "                \n",
    "                img_metadata = {\n",
    "                    'path': img_path,\n",
    "                    'size': (w, h),\n",
    "                    'is_grayscale': is_grayscale,\n",
    "                    'quadrants': quadrant_metadata\n",
    "                }\n",
    "                \n",
    "                if is_grayscale:\n",
    "                    chunk_grayscale.append(img_metadata)\n",
    "                else:\n",
    "                    chunk_normal.append(img_metadata)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Errore processando {img_path}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        chunk_results = process_chunk(chunk_normal, chunk_grayscale, yolo_model, yolo_model_grayscale,\n",
    "                                    batch_size_normal, batch_size_grayscale, expansion)\n",
    "        \n",
    "        final_results.update(chunk_results)\n",
    "        \n",
    "        processed_paths.update([metadata['path'] for metadata in chunk_normal + chunk_grayscale])\n",
    "        \n",
    "        if len(processed_paths) % checkpoint_every < chunk_size:\n",
    "            save_checkpoint(checkpoint_file, processed_paths, final_results)\n",
    "            print(f\"Checkpoint salvato: {len(processed_paths)} immagini processate\")\n",
    "        \n",
    "        del chunk_normal, chunk_grayscale, chunk_results\n",
    "        gc.collect()\n",
    "        if torch.backends.mps.is_available():\n",
    "            torch.mps.empty_cache()\n",
    "    \n",
    "    save_checkpoint(checkpoint_file, processed_paths, final_results)\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "def save_checkpoint(checkpoint_file, processed_paths, results):\n",
    "    checkpoint_data = {\n",
    "        'processed_paths': list(processed_paths),\n",
    "        'results': results,\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    with open(checkpoint_file, 'wb') as f:\n",
    "        pickle.dump(checkpoint_data, f)\n",
    "\n",
    "def process_chunk(normal_metadata, grayscale_metadata, yolo_model, yolo_model_grayscale, \n",
    "                 batch_size_normal, batch_size_grayscale, expansion=0.4):\n",
    "    all_detections = {}\n",
    "    \n",
    "    if normal_metadata:\n",
    "        for i in tqdm(range(0, len(normal_metadata), batch_size_normal), desc=\"Batch normali\", leave=False):\n",
    "            batch = normal_metadata[i:i+batch_size_normal]\n",
    "            batch_results = process_quadrants_batch(batch, yolo_model, 'mps', False)\n",
    "            \n",
    "            for img_path, detections in batch_results:\n",
    "                if img_path not in all_detections:\n",
    "                    all_detections[img_path] = []\n",
    "                all_detections[img_path].extend(detections)\n",
    "            \n",
    "            if torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "    \n",
    "    if grayscale_metadata:\n",
    "        for i in tqdm(range(0, len(grayscale_metadata), batch_size_grayscale), desc=\"Batch grayscale\", leave=False):\n",
    "            batch = grayscale_metadata[i:i+batch_size_grayscale]\n",
    "            batch_results = process_quadrants_batch(batch, yolo_model_grayscale, 'mps', True)\n",
    "            \n",
    "            for img_path, detections in batch_results:\n",
    "                if img_path not in all_detections:\n",
    "                    all_detections[img_path] = []\n",
    "                all_detections[img_path].extend(detections)\n",
    "    \n",
    "    detection_data_for_embeddings = {}\n",
    "    iou_threshold = 0.05\n",
    "    \n",
    "    for img_path in all_detections.keys():\n",
    "        pil_img = Image.open(img_path).convert(\"RGB\")\n",
    "        is_grayscale = detect_grayscale(pil_img)\n",
    "        \n",
    "        merged_detections = merge_overlapping_boxes(all_detections[img_path], threshold=iou_threshold)\n",
    "        \n",
    "        if is_grayscale:\n",
    "            pil_img = enhance_grayscale(pil_img)\n",
    "        \n",
    "        detection_args_list = []\n",
    "        for xmin, ymin, xmax, ymax, conf in merged_detections:\n",
    "            detection_args_list.append((img_path, pil_img, [xmin, ymin, xmax, ymax], conf, is_grayscale))\n",
    "        \n",
    "        detection_data_for_embeddings[img_path] = detection_args_list\n",
    "    \n",
    "    chunk_results = extract_embeddings_parallel(detection_data_for_embeddings, max_workers=4, expansion=expansion)\n",
    "    \n",
    "    return chunk_results\n",
    "\n",
    "\n",
    "CHECKPOINT_FILE = \"processing_checkpoint_70k.pkl\"\n",
    "BATCH_SIZE = 16\n",
    "CHECKPOINT_EVERY = 500  \n",
    "\n",
    "person_detections_per_image = process_images_optimized(\n",
    "    batch_size_normal=BATCH_SIZE,\n",
    "    batch_size_grayscale=BATCH_SIZE,\n",
    "    checkpoint_every=CHECKPOINT_EVERY,\n",
    "    checkpoint_file=CHECKPOINT_FILE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f901cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = dict()\n",
    "\n",
    "\n",
    "for image_path in all_image_paths:\n",
    "    if image_path not in person_detections_per_image:\n",
    "        to_save[image_path] = []\n",
    "        continue\n",
    "    values = person_detections_per_image[image_path]\n",
    "    to_save[image_path] = values.copy()\n",
    "    \n",
    "    for val in to_save[image_path]:\n",
    "        val[\"crop\"] = None\n",
    "        val[\"original\"] = None\n",
    "\n",
    "    \n",
    "\n",
    "with open(\"person_detections_per_image.pkl\", \"wb\") as f:\n",
    "    pickle.dump(to_save, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d0b640",
   "metadata": {},
   "source": [
    "#### Cluster Division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d8c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"person_detections_per_image.pkl\", \"rb\") as f:\n",
    "    person_detections_per_image = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660af985",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for i, key in tqdm(enumerate(person_detections_per_image), total=len(person_detections_per_image)):\n",
    "    values = person_detections_per_image[key]\n",
    "    values_sorted = sorted(\n",
    "        values,\n",
    "        key=lambda v: (v[\"bbox\"][2] - v[\"bbox\"][0]) * (v[\"bbox\"][3] - v[\"bbox\"][1]),\n",
    "        reverse=True \n",
    "    )\n",
    "\n",
    "    if len(values_sorted) == 0:\n",
    "        continue\n",
    "\n",
    "    for val in values_sorted:\n",
    "        features.append(val[\"embedding\"])\n",
    "        val[\"feature index\"] = len(features) - 1    \n",
    "        val[\"path\"] = all_image_paths[i]\n",
    "\n",
    "\n",
    "features = np.array(features)\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_std = StandardScaler().fit_transform(features)\n",
    "pca = PCA(n_components=min(features.shape[0], 20), random_state=42)\n",
    "features_pca = pca.fit_transform(features_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af9813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the perfect number of clusters - recommended anyway to consider a high number of clusters\n",
    "\n",
    "inertias = []\n",
    "K_range = range(4, 16, 2)\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(features_pca)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "    labels_tmp = km.labels_\n",
    "    n_labels = len(set(labels_tmp))\n",
    "\n",
    "    if n_labels > 1 and n_labels < len(features_pca):\n",
    "        score = silhouette_score(features_pca, labels_tmp)\n",
    "    else:\n",
    "        score = float('nan')\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "plt.plot(K_range, inertias, 'o-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow method')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(K_range, silhouette_scores, 'o-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084337b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 270 \n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "labels_persons = kmeans.fit_predict(features_pca)\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "dists = cdist(centroids, centroids)\n",
    "\n",
    "linkage_matrix = linkage(centroids, method='ward')\n",
    "ordered_cluster_ids = leaves_list(linkage_matrix) \n",
    "\n",
    "label_map = {old: new for new, old in enumerate(ordered_cluster_ids)}\n",
    "labels_persons = np.array([label_map[label] for label in labels_persons])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aade8712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "output_dir = \"clusters_kmeans_1000\"\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "    \n",
    "os.makedirs(output_dir)\n",
    "\n",
    "index_to_cluster = {i: label for i, label in enumerate(labels_persons)}\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "\n",
    "for key, values in person_detections_per_image.items():\n",
    "    if len(values) == 0:\n",
    "        continue\n",
    "    for val in values:\n",
    "        cluster_id = index_to_cluster.get(val[\"feature index\"])\n",
    "        if cluster_id is not None:\n",
    "            clusters[cluster_id].append((val, key))\n",
    "\n",
    "for cluster_id, items in tqdm(clusters.items(), desc=\"Saving clusters\"):\n",
    "    cluster_folder = os.path.join(output_dir, f\"cluster_{cluster_id}\")\n",
    "    os.makedirs(cluster_folder, exist_ok=True)\n",
    "\n",
    "    for idx, (val, key) in enumerate(items):\n",
    "        img = Image.open(val[\"path\"])\n",
    "        xmin, ymin, xmax, ymax = val[\"bbox\"]\n",
    "        crop = img.crop((\n",
    "            max(0, xmin - 0.15 * (xmax - xmin)),\n",
    "            max(0, ymin - 0.15 * (ymax - ymin)),\n",
    "            min(img.width, xmax + 0.15 * (xmax - xmin)),\n",
    "            min(img.height, ymax + 0.15 * (ymax - ymin))\n",
    "        ))\n",
    "\n",
    "        filename = os.path.join(cluster_folder, os.path.basename(val[\"path\"]).replace(\".jpg\", \"\") + f\"_{idx}.jpg\")\n",
    "        \n",
    "        val[\"path_crop\"] = filename\n",
    "        \n",
    "        crop.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d36ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = []\n",
    "for root, _, files in os.walk(\"clusters_kmeans_1000\"):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            try:\n",
    "\n",
    "                im.append((None, os.path.join(root, file)))\n",
    "            except ValueError:\n",
    "                continue\n",
    "im.sort()\n",
    "\n",
    "alm = [p for _, p in im]\n",
    "\n",
    "print(len(alm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc846e",
   "metadata": {},
   "source": [
    "*To clean up the automated results, configure the clusters_kmeans_1000 folder in revisor.js and disable group mode and annotations. Remove misclassified poses, move elements between different categories, and keep only semantically coherent clusters. These will become the building blocks of subsequent steps.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbdc22b",
   "metadata": {},
   "source": [
    "### STEP 2: Leveraging the clusters created previously, I divide the images by activities that people are doing\n",
    "\n",
    "A scene is not simply the sum of the poses present: a person sitting alone has a different meaning from two people sitting and conversing. Each image is represented as a vector of detected poses, capturing both the type and number of people involved.\n",
    "\n",
    "The ordering by size of people reflects the assumption that the main actors are more visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories(path):\n",
    "    directories = []\n",
    "    for item in os.listdir(path):\n",
    "        item_path = os.path.join(path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            directories.append(item_path)\n",
    "    return directories\n",
    "\n",
    "number_path = {path.split(\"/\")[-1]: path for path in get_directories(\"clusters_kmeans_1000\")}\n",
    "\n",
    "def sort_key(path):\n",
    "    try:\n",
    "        return int(path.split(\"/\")[-1].split(\"_\")[-1])\n",
    "    except ValueError:\n",
    "        return float('inf')\n",
    "\n",
    "sorted_directories = sorted(number_path.keys(), key=sort_key)\n",
    "sorted_directories = [x for x in sorted_directories if \"undefined\" not in x]\n",
    "\n",
    "\n",
    "features_indexes = {cluster_path: i for i, cluster_path in enumerate(sorted_directories)}\n",
    "\n",
    "path_cluster = dict()\n",
    "\n",
    "def concat_list_elements_to_string(lst):\n",
    "    string = \"\"\n",
    "    for element in lst:\n",
    "        string += str(element) + \"_\"\n",
    "    return string[:-1]  \n",
    "\n",
    "def get_image_size(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            width, height = img.size\n",
    "            return width * height\n",
    "    except Exception as e:\n",
    "        print(f\"Errore nel leggere l'immagine {image_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "temp_cluster_data = dict()\n",
    "\n",
    "for cluster_path in sorted_directories:\n",
    "    for image_path in os.listdir(number_path[cluster_path]):\n",
    "        if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            zero_image_path = concat_list_elements_to_string(image_path.split(\"/\")[-1].split(\"_\")[:-1])+\".\"+image_path.split(\"/\")[-1].split(\".\")[1]\n",
    "            full_image_path = os.path.join(number_path[cluster_path], image_path)\n",
    "            image_size = get_image_size(full_image_path)\n",
    "            cluster_info = (features_indexes[cluster_path], image_size, full_image_path)\n",
    "            \n",
    "            if zero_image_path in temp_cluster_data.keys():\n",
    "                temp_cluster_data[zero_image_path].append(cluster_info)\n",
    "            else:\n",
    "                temp_cluster_data[zero_image_path] = [cluster_info]\n",
    "\n",
    "for zero_image_path, cluster_infos in temp_cluster_data.items():\n",
    "    sorted_infos = sorted(cluster_infos, key=lambda x: x[1], reverse=True)\n",
    "    path_cluster[os.path.join(PATH_IMAGES, zero_image_path)] = [info[0] for info in sorted_infos]\n",
    "\n",
    "            \n",
    "for image_path in all_image_paths:\n",
    "    if image_path not in path_cluster.keys():\n",
    "        path_cluster[image_path] = []\n",
    "\n",
    "features = []\n",
    "img_path_feature = []\n",
    "\n",
    "n_max_detections = max([len(v) for v in path_cluster.values()])\n",
    "BASE_LISTS_PERSONS = [-1 for _ in range(n_max_detections) ]\n",
    "\n",
    "for key in tqdm(path_cluster.keys(), desc=\"Preparing features\"):\n",
    "    values = path_cluster[key]\n",
    "\n",
    "    list_ = BASE_LISTS_PERSONS.copy()\n",
    "    for i, val in enumerate(values):\n",
    "        if i < len(list_): list_[i] = val\n",
    "\n",
    "    if len(values) <= 0:\n",
    "        print(\"0 VALUES\")\n",
    "\n",
    "    features.append(list_)\n",
    "    img_path_feature.append(key)\n",
    "\n",
    "features = np.array(features)\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2199fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_std = StandardScaler().fit_transform(features)\n",
    "pca = PCA(n_components=min(features.shape[0], 20, features.shape[1]), random_state=42)\n",
    "features_pca = pca.fit_transform(features_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b154af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the perfect number of clusters - recommended anyway to consider a high number of clusters\n",
    "\n",
    "inertias = []\n",
    "K_range = range(1, 11, 1)\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    km.fit(features_pca)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "    labels_tmp = km.labels_\n",
    "    n_labels = len(set(labels_tmp))\n",
    "\n",
    "    if n_labels > 1 and n_labels < len(features_pca):\n",
    "        score = silhouette_score(features_pca, labels_tmp)\n",
    "    else:\n",
    "        score = float('nan')\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "plt.plot(K_range, inertias, 'o-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow method')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(K_range, silhouette_scores, 'o-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dcd72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 500 \n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "labels_multi_persons = kmeans.fit_predict(features_pca)\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "dists = cdist(centroids, centroids)\n",
    "\n",
    "linkage_matrix = linkage(centroids, method='ward')\n",
    "ordered_cluster_ids = leaves_list(linkage_matrix) \n",
    "\n",
    "label_map = {old: new for new, old in enumerate(ordered_cluster_ids)}\n",
    "labels_multi_persons = np.array([label_map[label] for label in labels_multi_persons])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0414bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"clusters_kmeans_500_multi\"\n",
    "\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i, label in tqdm(enumerate(labels_multi_persons), desc=\"Creating clusters\", total=len(labels_multi_persons)):\n",
    "    cluster_folder = os.path.join(output_dir, f\"cluster_{label}\")\n",
    "    os.makedirs(cluster_folder, exist_ok=True)\n",
    "\n",
    "    shutil.copy(img_path_feature[i], os.path.join(cluster_folder, os.path.basename(img_path_feature[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab088357",
   "metadata": {},
   "source": [
    "*Configure clusters_kmeans_500_multi keeping advanced modes disabled. Verify that each cluster represents a recurring domestic situation like watching TV, cooking, working at computer. Move images that don't match the dominant pattern of the cluster.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ebad9e",
   "metadata": {},
   "source": [
    "### STEP 3: I divide into clusters the small groups in images of 3 by 3, furthermore I add other features\n",
    "\n",
    "Consecutive triplets of images capture complete activity sequences: cooking is not just a person at the stove, but preparation, cooking and cleaning. Home Assistant data adds the necessary environmental context to disambiguate visually identical situations: watching TV with lights off in the evening is a different routine compared to the afternoon with natural lighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9a5ef6",
   "metadata": {},
   "source": [
    "*The CSV file contains temporal data synchronized from the Home Assistant system: device states and room sensors. This data is correlated with images by timestamp, creating a multimodal context.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef499e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv = pd.read_csv(PATH_CSV)\n",
    "\n",
    "csv['timestamp'] = pd.to_datetime(csv['timestamp'])\n",
    "\n",
    "def parse_image_timestamp(path):\n",
    "    filename = path.split(\"/\")[-1].replace(\".jpg\", \"\")\n",
    "    dt = datetime.strptime(filename, \"%d-%m_%H-%M-%S\")\n",
    "    dt = dt.replace(year=2025)\n",
    "    return dt\n",
    "\n",
    "info_dict = {}\n",
    "\n",
    "for path in tqdm(all_image_paths):\n",
    "    image_time = parse_image_timestamp(path)\n",
    "    csv['delta'] = (csv['timestamp'] - image_time).abs()\n",
    "    candidates = csv[csv['delta'] <= timedelta(milliseconds=750)]\n",
    "\n",
    "    if not candidates.empty:\n",
    "        closest_row = candidates.sort_values('delta').iloc[0]\n",
    "        info_dict[path] = closest_row.drop('delta').to_dict()\n",
    "    else:\n",
    "        info_dict[path] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d352931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "keys = list(next(v for v in info_dict.values() if v is not None).keys())\n",
    "encoders = {k: LabelEncoder() for k in keys if k != 'timestamp'}\n",
    "\n",
    "for k in encoders:\n",
    "    values = [info[k] for info in info_dict.values() if info is not None]\n",
    "    encoders[k].fit(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e20b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_dirs(path):\n",
    "    return [os.path.join(path, d) for d in os.listdir(path) \n",
    "            if os.path.isdir(os.path.join(path, d)) and d.startswith(\"cluster_\")]\n",
    "\n",
    "def extract_cluster_num(path):\n",
    "    return int(path.split(\"_\")[-1])\n",
    "\n",
    "def get_original_name(filename):\n",
    "    return \"_\".join(filename.split(\"_\")[:-1]) + \".\" + filename.split(\".\")[-1]\n",
    "\n",
    "def get_image_size(path):\n",
    "    try:\n",
    "        with Image.open(path) as img:\n",
    "            return img.size[0] * img.size[1]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "img_path_feature = all_image_paths.copy()\n",
    "\n",
    "output_dir = \"clusters_kmeans_500_multi\"\n",
    "cluster_dirs = get_cluster_dirs(output_dir)\n",
    "\n",
    "image_clusters = {}\n",
    "\n",
    "for cluster_dir in cluster_dirs:\n",
    "    cluster_label = extract_cluster_num(cluster_dir)\n",
    "    \n",
    "    for filename in os.listdir(cluster_dir):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            original_name = get_original_name(filename)\n",
    "            full_path = os.path.join(cluster_dir, filename)\n",
    "            size = get_image_size(full_path)\n",
    "            \n",
    "            cluster_info = (cluster_label, size, full_path)\n",
    "            \n",
    "            if original_name in image_clusters:\n",
    "                image_clusters[original_name].append(cluster_info)\n",
    "            else:\n",
    "                image_clusters[original_name] = [cluster_info]\n",
    "\n",
    "path_to_cluster = {}\n",
    "for original_name, infos in image_clusters.items():\n",
    "    best_cluster = max(infos, key=lambda x: x[1])[0]\n",
    "    full_path = os.path.join(PATH_IMAGES, original_name)\n",
    "    path_to_cluster[full_path] = best_cluster\n",
    "\n",
    "labels_multi_persons = []\n",
    "for img_path in img_path_feature:\n",
    "    labels_multi_persons.append(path_to_cluster.get(img_path, -1))\n",
    "\n",
    "with open(\"person_detections_per_image.pkl\", \"rb\") as f:\n",
    "    person_detections_per_image = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f4ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "\n",
    "def extract_temporal_features(timestamp):\n",
    "    hour = timestamp.hour\n",
    "    weekday = timestamp.weekday()\n",
    "\n",
    "    month = timestamp.month\n",
    "    if month in [12, 1, 2]:\n",
    "        season = 0  # winter\n",
    "    elif month in [3, 4, 5]:\n",
    "        season = 1  # spring\n",
    "    elif month in [6, 7, 8]:\n",
    "        season = 2  # summer\n",
    "    else:\n",
    "        season = 3  # autumn\n",
    "\n",
    "    return hour, weekday, season\n",
    "\n",
    "timestamps = [dt for dt, _ in image_info]\n",
    "paths = all_image_paths\n",
    "\n",
    "grouped_triplets = []\n",
    "tolerance = timedelta(seconds=30)\n",
    "for i in range(2, len(timestamps)):\n",
    "    t0 = timestamps[i]\n",
    "    t1, t2 = t0 - timedelta(minutes=1), t0 - timedelta(minutes=2)\n",
    "\n",
    "    idx1 = max(0, bisect_left(timestamps, t1) - 1)\n",
    "    while idx1 > 0 and abs(timestamps[idx1] - t1) > abs(timestamps[idx1 - 1] - t1):\n",
    "        idx1 -= 1\n",
    "    idx2 = max(0, bisect_left(timestamps, t2) - 1)\n",
    "    while idx2 > 0 and abs(timestamps[idx2] - t2) > abs(timestamps[idx2 - 1] - t2):\n",
    "        idx2 -= 1\n",
    "    if abs(timestamps[idx1] - t1) <= tolerance and abs(timestamps[idx2] - t2) <= tolerance:\n",
    "        grouped_triplets.append([paths[idx2], paths[idx1], paths[i]])\n",
    "\n",
    "scene_histograms = []\n",
    "scene_groups = []\n",
    "\n",
    "for group in tqdm(grouped_triplets, desc=\"Building histograms\"):\n",
    "    hist = []\n",
    "    for img_path in group:\n",
    "        # scene_emb = extract_scene_embedding(img_path) # TO USE ALSO THE SCENE EMBEDDING\n",
    "        img_idx = img_path_feature.index(img_path) if img_path in img_path_feature else -1\n",
    "        person_cluster_label = labels_multi_persons[img_idx] if img_idx >= 0 else -1\n",
    "        num_persons = len(person_detections_per_image.get(img_path, []))\n",
    "\n",
    "        # combined_features = np.concatenate([\n",
    "        #     scene_emb,                    # 7 dimensioni68\n",
    "        #     [person_cluster_label]*20,       # 20 dimensione  \n",
    "        #     [num_persons]*20                 # 20 dimensione\n",
    "        # ]) -> WHEN IS USED ALSO scene_emb\n",
    "\n",
    "        feature_list = [person_cluster_label, num_persons] * 5\n",
    "        csv_info = info_dict.get(img_path)\n",
    "\n",
    "\n",
    "        if csv_info is not None:\n",
    "            ts = pd.to_datetime(csv_info['timestamp'])\n",
    "            hour, weekday, season = extract_temporal_features(ts)\n",
    "            feature_list.extend([hour, weekday])\n",
    "\n",
    "            for k in keys:\n",
    "                if k in [\"timestamp\", \"light.lampada\" ,\"light.letto\",\"light.scrivania\",\"switch.monitor\", \"delta\", \"sensor.luminosita_camera\"]:\n",
    "                    continue\n",
    "                v = csv_info[k]\n",
    "                if k in encoders:\n",
    "                    encoded_val = encoders[k].transform([v])[0]\n",
    "                    feature_list.append(encoded_val)\n",
    "                else:\n",
    "                    feature_list.append(float(v))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        combined_features = np.array(feature_list)\n",
    "\n",
    "\n",
    "        hist.append(combined_features)\n",
    "        \n",
    "\n",
    "    if len(hist) != 3:\n",
    "        continue\n",
    "        \n",
    "    scene_histograms.append(np.concatenate(hist))\n",
    "    scene_groups.append(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69ce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.stack(scene_histograms)\n",
    "print(H.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "H_scaled = scaler.fit_transform(H)\n",
    "\n",
    "pca = PCA(n_components=20, random_state=42)\n",
    "H_pca = pca.fit_transform(H_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the perfect number of clusters - recommended anyway to consider a high number of clusters\n",
    "\n",
    "K_range_scenes = range(10, 201, 5)  # es. [10,20,...,100]\n",
    "inertias_scenes = []\n",
    "silhouette_scenes = []\n",
    "\n",
    "for k in K_range_scenes:\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10).fit(H_pca)\n",
    "    inertias_scenes.append(km.inertia_)\n",
    "    if k > 1 and k < len(H_pca):\n",
    "        labels_s = km.labels_\n",
    "        silhouette_scenes.append(silhouette_score(H_pca, labels_s))\n",
    "    else:\n",
    "        silhouette_scenes.append(np.nan)\n",
    "\n",
    "# Plot Elbow \n",
    "plt.figure()\n",
    "plt.plot(list(K_range_scenes), inertias_scenes, 'o-')\n",
    "plt.xlabel('k (scene clusters)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method — Level 2')\n",
    "plt.show()\n",
    "\n",
    "# Plot Silhouette\n",
    "plt.figure()\n",
    "plt.plot(list(K_range_scenes), silhouette_scenes, 'o-')\n",
    "plt.xlabel('k (scene clusters)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis — Level 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 300 \n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "labels_scenes = kmeans.fit_predict(H_pca)\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "dists = cdist(centroids, centroids)\n",
    "\n",
    "linkage_matrix = linkage(centroids, method='ward')\n",
    "ordered_cluster_ids = leaves_list(linkage_matrix) \n",
    "\n",
    "label_map = {old: new for new, old in enumerate(ordered_cluster_ids)}\n",
    "labels_scenes = np.array([label_map[label] for label in labels_scenes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662e75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"cluster_triplet\", ignore_errors=True)\n",
    "os.makedirs(\"cluster_triplet\", exist_ok=True)\n",
    "\n",
    "for idx, (group, lbl) in tqdm(enumerate(zip(scene_groups, labels_scenes)), total=len(scene_groups)):\n",
    "    out_dir = os.path.join(\"cluster_triplet\", f\"cluster_{lbl}\", f\"group_{idx}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for img in group:\n",
    "        shutil.copy(img, os.path.join(out_dir, os.path.basename(img)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e4a81f",
   "metadata": {},
   "source": [
    "*Activate group mode and annotations in revisor.js and configure cluster_triplet. Assign semantic labels to routines (\"family breakfast\", \"evening work\") and configure automatic actions for each scenario. Each cluster must represent a complete behavioral pattern with its environmental triggers.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
