{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1637a6f8",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "##### Explanation:\n",
    "\n",
    "The provided code concerns the training of an AI model for human activity recognition within a smart home context. The model is designed to analyze images using YOLO (You Only Look Once) for person detection, and then use an EfficientNet-based classifier to identify activities. Finally, temporal and sensor data is analyzed and classified via a dataset containing both visual and contextual information. This culminates in the creation of an AI model capable of recognizing activity sequences within a home environment, enabling smarter home management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Algorithm Diagram](algorithm_diagram.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b070b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics==8.3.165 torch==2.3.0 timm==1.0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78071e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import json\n",
    "import tempfile\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import timm\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b297e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' #  'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef44bd7f",
   "metadata": {},
   "source": [
    "## STEP 1: person recognition model\n",
    "\n",
    "In this step, the model uses YOLO to detect people in images. The initially fine-tuned YOLO version is based on YOLOv8n, which was further improved using the bounding boxes (bboxes) detected by YOLOv12l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c05cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_path = \"clusters_kmeans_1000\"\n",
    "pickle_path = \"person_detections_per_image.pkl\"\n",
    "images_path = \"Home Assistant Monitor Complete Jul 12 2025/images\"\n",
    "\n",
    "n_images = 500\n",
    "\n",
    "with open(pickle_path, \"rb\") as f:\n",
    "    all_data = pickle.load(f)\n",
    "\n",
    "all_data = {path: [{\"bbox\": x[\"bbox\"]} for x in all_data[path]] for path in all_data.keys()}\n",
    "\n",
    "def getbasicname(path): # path ex: '11-07_13-43-46_106.jpg'\n",
    "    return \"_\".join(x for x in path.split(\"_\")[:-1]) + \".\"+ path.split(\".\")[-1]\n",
    "\n",
    "bbox = {}\n",
    "\n",
    "while len(list(bbox.keys())) < n_images:\n",
    "    dirs = os.listdir(img_path)\n",
    "\n",
    "    for dir in dirs:\n",
    "        if os.path.isfile(os.path.join(img_path, dir)):\n",
    "            continue\n",
    "        \n",
    "        path = random.choice(os.listdir(os.path.join(img_path, dir)))\n",
    "\n",
    "        basic = getbasicname(path)\n",
    "\n",
    "        if os.path.join(images_path, basic) in list(bbox.keys()) or len(all_data[os.path.join(images_path, basic)]) == 0:\n",
    "            continue\n",
    "\n",
    "        bbox[os.path.join(images_path, basic)] = [\n",
    "            x[\"bbox\"] for x in all_data[os.path.join(images_path, basic)]\n",
    "        ]\n",
    "        \n",
    " \n",
    "        if len(list(bbox.keys())) > n_images:\n",
    "            break\n",
    "\n",
    "\n",
    "OUT_ROOT = \"ultralytics_dataset\"\n",
    "IMAGES_TRAIN = os.path.join(OUT_ROOT, \"images\", \"train\")\n",
    "IMAGES_VAL   = os.path.join(OUT_ROOT, \"images\", \"val\")\n",
    "LABELS_TRAIN = os.path.join(OUT_ROOT, \"labels\", \"train\")\n",
    "LABELS_VAL   = os.path.join(OUT_ROOT, \"labels\", \"val\")\n",
    "\n",
    "os.makedirs(IMAGES_TRAIN, exist_ok=True)\n",
    "os.makedirs(IMAGES_VAL, exist_ok=True)\n",
    "os.makedirs(LABELS_TRAIN, exist_ok=True)\n",
    "os.makedirs(LABELS_VAL, exist_ok=True)\n",
    "\n",
    "val_ratio = 0.2\n",
    "\n",
    "all_paths = list(bbox.keys())\n",
    "if len(all_paths) == 0:\n",
    "    raise RuntimeError(\"bbox is empty\")\n",
    "\n",
    "random.shuffle(all_paths)\n",
    "selected_paths = all_paths[:n_images] if len(all_paths) >= n_images else all_paths\n",
    "\n",
    "n_total = len(selected_paths)\n",
    "n_val = int(n_total * val_ratio)\n",
    "n_train = n_total - n_val\n",
    "\n",
    "train_paths = selected_paths[:n_train]\n",
    "val_paths   = selected_paths[n_train:n_train + n_val]\n",
    "\n",
    "print(f\"Total: {n_total} (train={len(train_paths)}, val={len(val_paths)})\")\n",
    "\n",
    "def sanitize_bbox(bbox, iw, ih):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    x1 = max(0, min(iw-1, float(x1)))\n",
    "    y1 = max(0, min(ih-1, float(y1)))\n",
    "    x2 = max(0, min(iw-1, float(x2)))\n",
    "    y2 = max(0, min(ih-1, float(y2)))\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return None\n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "def bbox_to_yolo_line(bbox, iw, ih, cls_id=0):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    x_c = ((x1 + x2) / 2.0) / iw\n",
    "    y_c = ((y1 + y2) / 2.0) / ih\n",
    "    w = (x2 - x1) / iw\n",
    "    h = (y2 - y1) / ih\n",
    "\n",
    "    x_c = min(max(x_c, 0.0), 1.0)\n",
    "    y_c = min(max(y_c, 0.0), 1.0)\n",
    "    w   = min(max(w,   0.0), 1.0)\n",
    "    h   = min(max(h,   0.0), 1.0)\n",
    "    return f\"{cls_id} {x_c:.6f} {y_c:.6f} {w:.6f} {h:.6f}\"\n",
    "\n",
    "def safe_copy(src, dst_dir):\n",
    "    base = os.path.basename(src)\n",
    "    dst = os.path.join(dst_dir, base)\n",
    "    if not os.path.exists(dst):\n",
    "        shutil.copy2(src, dst)\n",
    "        return dst\n",
    "    name, ext = os.path.splitext(base)\n",
    "    i = 1\n",
    "    while True:\n",
    "        new_name = f\"{name}_{i}{ext}\"\n",
    "        dst = os.path.join(dst_dir, new_name)\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.copy2(src, dst)\n",
    "            return dst\n",
    "        i += 1\n",
    "\n",
    "def save_image_and_label(src_img_path, bbox_list, images_dir, labels_dir):\n",
    "    dst_img_path = safe_copy(src_img_path, images_dir)\n",
    "    dst_base = os.path.splitext(os.path.basename(dst_img_path))[0]\n",
    "    label_path = os.path.join(labels_dir, dst_base + \".txt\")\n",
    "\n",
    "    iw, ih = Image.open(src_img_path).size\n",
    "    lines = []\n",
    "    for b in bbox_list:\n",
    "        sb = sanitize_bbox(b, iw, ih)\n",
    "        if sb is None:\n",
    "            continue\n",
    "        lines.append(bbox_to_yolo_line(sb, iw, ih, cls_id=0))\n",
    "    with open(label_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "for p in train_paths:\n",
    "    save_image_and_label(p, bbox[p], IMAGES_TRAIN, LABELS_TRAIN)\n",
    "\n",
    "for p in val_paths:\n",
    "    save_image_and_label(p, bbox[p], IMAGES_VAL, LABELS_VAL)\n",
    "\n",
    "dataset_yaml_path = os.path.join(OUT_ROOT, \"dataset.yaml\")\n",
    "with open(dataset_yaml_path, \"w\") as f:\n",
    "    f.write(f\"train: {os.path.abspath(IMAGES_TRAIN)}\\n\")\n",
    "    f.write(f\"val:   {os.path.abspath(IMAGES_VAL)}\\n\")\n",
    "    f.write(\"nc: 1\\n\")\n",
    "    f.write(\"names: ['person']\\n\")\n",
    "\n",
    "print(\"Dataset created in\", os.path.abspath(OUT_ROOT))\n",
    "print(\" - images train:\", len(os.listdir(IMAGES_TRAIN)))\n",
    "print(\" - images val:  \", len(os.listdir(IMAGES_VAL)))\n",
    "print(\"YAML written in:\", dataset_yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10767194",
   "metadata": {},
   "source": [
    "Subsequently to train the model (in the same carpet of this file):\n",
    "\n",
    "```\n",
    "yolo task=detect mode=train model=yolov8n.pt data=ultralytics_dataset/dataset.yaml epochs=50 imgsz=640 batch=16\n",
    "```\n",
    "\n",
    "and save the model in this carpet with the name of \"yolo-domotica-ai.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOPersonDetector:\n",
    "    def __init__(self, weights_path: str, device: str = 'cpu', person_class: int = 0, iou: float = 0.5):\n",
    "        self.model = YOLO(weights_path)\n",
    "        self.model.to(device)\n",
    "        self.device = torch.device(device)\n",
    "        self.person_class = person_class\n",
    "        self.iou = iou\n",
    "        self.num_classes = 1  \n",
    "        self._weights_path = weights_path\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_numpy(image_tensor: torch.Tensor) -> np.ndarray:\n",
    "        # sposta sempre su CPU prima di convertire\n",
    "        image_tensor = image_tensor.detach().cpu()\n",
    "\n",
    "        arr = image_tensor\n",
    "        if arr.dtype != torch.uint8:\n",
    "            arr = arr.clamp(0, 255)\n",
    "            if arr.max() <= 1.0:\n",
    "                arr = arr * 255.0\n",
    "            arr = arr.byte()\n",
    "        arr = arr.permute(1, 2, 0).contiguous().numpy()  # CHW -> HWC\n",
    "        return arr\n",
    "    \n",
    "    def predict(self, image: torch.Tensor, score_threshold: float = 0.5):\n",
    "        np_img = self._to_numpy(image)\n",
    "\n",
    "        results = self.model.predict(\n",
    "            source=np_img, conf=score_threshold, iou=self.iou, verbose=False, device=self.device\n",
    "        )\n",
    "        res = results[0]\n",
    "        if res.boxes is None or len(res.boxes) == 0:\n",
    "            return {'boxes': torch.empty((0,4), dtype=torch.float32),\n",
    "                    'scores': torch.empty((0,), dtype=torch.float32)}\n",
    "        boxes_xyxy = res.boxes.xyxy  # (N,4) su device del modello\n",
    "        confs = res.boxes.conf       # (N,)\n",
    "        clss = res.boxes.cls         # (N,)\n",
    "\n",
    "        keep = (clss == self.person_class)\n",
    "        boxes_xyxy = boxes_xyxy[keep].detach().cpu()\n",
    "        confs = confs[keep].detach().cpu()\n",
    "        return {'boxes': boxes_xyxy, 'scores': confs}\n",
    "    def save_weights(self, path: str):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        if hasattr(self, \"_weights_path\") and os.path.isfile(self._weights_path):\n",
    "            shutil.copy2(self._weights_path, path)\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Non trovo il file dei pesi YOLO di origine: {getattr(self, '_weights_path', None)}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052e4b35",
   "metadata": {},
   "source": [
    "## STEP 2\n",
    "\n",
    "The second step involves training an activity classification model using segmented images and sensor data. The EfficientNet model is used as the backbone for feature extraction, while a softmax classifier is used to predict which activity is being performed based on the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847776bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivityClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.backbone = timm.create_model(\n",
    "            'efficientnet_b0', pretrained=True,\n",
    "            num_classes=0, global_pool=''\n",
    "        )\n",
    "        self.backbone.to(self.device)\n",
    "        self.backbone.eval()\n",
    "\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        embed_dim = 1280\n",
    "\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes).to(self.device)\n",
    "\n",
    "        self.preprocess = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.classifier.parameters(), lr=1e-3)\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.backbone(images)\n",
    "            pooled = self.adaptive_pool(features)\n",
    "            embeddings = pooled.flatten(1)\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "    def predict(self, image: Image.Image, return_prob=False):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            img_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "            logits = self.forward(img_tensor)\n",
    "            if return_prob:\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                return probs.argmax(dim=1).item(), probs.squeeze()\n",
    "            else:\n",
    "                return logits.argmax(dim=1).item()\n",
    "\n",
    "    def save_weights(self, path=\"activity_classifier_weights.pth\"):\n",
    "        torch.save({\n",
    "            'num_classes': self.num_classes,\n",
    "            'state_dict': self.classifier.state_dict()\n",
    "        }, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_file(cls, path, device='cpu'):\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        num_classes = checkpoint['num_classes']\n",
    "        model = cls(num_classes=num_classes, device=device)\n",
    "        model.classifier.load_state_dict(checkpoint['state_dict'])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5e921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilteredImageFolder(ImageFolder):\n",
    "    def find_classes(self, directory):\n",
    "        classes, class_to_idx = super().find_classes(directory)\n",
    "        classes = [cls for cls in classes if cls != \"undefined\"]\n",
    "        class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "\n",
    "dataset  = FilteredImageFolder(\n",
    "    root='clusters_kmeans_500_multi',\n",
    "    transform=ActivityClassifier(1).preprocess\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(dataset)) \n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d570798",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActivityClassifier(num_classes=len(train_dataset.dataset.classes), device=device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f7cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(20): # 20 hours more or less\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(enumerate(train_dataloader), desc=f\"Epoch {epoch+1} [Train]\", total=len(train_dataloader))\n",
    "    for i, (images, labels) in progress_bar:\n",
    "        images = images.to(model.device)\n",
    "        labels = labels.to(model.device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = model.criterion(outputs, labels)\n",
    "\n",
    "        model.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / (i + 1)\n",
    "        progress_bar.set_postfix(loss=loss.item(), avg_loss=avg_train_loss)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(enumerate(val_dataloader), desc=f\"Epoch {epoch+1} [Val]\", total=len(val_dataloader))\n",
    "        for i, (images, labels) in val_bar:\n",
    "            images = images.to(model.device)\n",
    "            labels = labels.to(model.device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = model.criterion(outputs, labels)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            avg_val_loss = total_val_loss / (i + 1)\n",
    "            val_bar.set_postfix(loss=loss.item(), avg_loss=avg_val_loss)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        model.save_weights(path=\"activity_classifier_weights_best.pth\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss = {avg_train_loss:.4f} | \"\n",
    "          f\"Val Loss = {avg_val_loss:.4f} | \"\n",
    "          f\"Best Val Loss = {best_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaff5d3",
   "metadata": {},
   "source": [
    "## STEP 3\n",
    "\n",
    "DomoticaAI model is designed to predict complex actions based on a combination of visual inputs (images) and sensory data (sensor readings), which are integrated to provide an accurate prediction of actions.\n",
    "\n",
    "### Detailed Model Functionality\n",
    "The DomoticaAI model is a complex neural network that integrates data from multiple sources to predict which action is about to be performed in a home environment. The flow of data and the model's architecture are designed to handle temporal sequences of images, sensors, and previous actions, enabling the model to learn daily routines.\n",
    "\n",
    "Model Structure:\n",
    "\n",
    "- Person Detector: the model starts with the YOLO person detector (pre-trained) to identify people in the images. \n",
    "- Activity Classifier\n",
    "- Sensor Data Embeddings: data from sensors (e.g., temperature, brightness) is transformed through a fully connected (Linear) layer that maps the sensory data into an embedding space. This allows the model to understand how environmental conditions affect people's activities.\n",
    "- LSTM Networks (Long Short-Term Memory): are used to process the temporal sequence of images and sensory data, allowing the model to \"remember\" previous steps and use them to predict the next step. LSTMs are critical for understanding the flow of activities over time (e.g., transitioning from \"cooking\" to \"cleaning\").\n",
    "\n",
    "### Detailed Architecture\n",
    "Here's a breakdown of the DomoticaAI architecture:\n",
    "\n",
    "1. Input: Images and Sensor Data\n",
    "    - Images are provided in temporal sequences. For each time step, an image is processed through the Person Detector (YOLO).\n",
    "    - Sensor data (e.g., temperature, brightness) is associated with the images and transformed into a numerical format, which is then passed to the network.\n",
    "2. Image Embeddings: each image is passed through an embedding layer that represents the activity of the detected person. This layer maps activity classes (e.g., \"sitting\", \"walking\") into a high-dimensional space.\n",
    "3. Image Temporal LSTM: the images are processed sequentially through an LSTM, which \"remembers\" information from previous images. This provides a temporal representation of the activity, considering the context of prior images.\n",
    "4. Action Temporal LSTM: additionally, LSTMs are used to handle previous actions, allowing the model to \"track\" past actions, which helps in predicting future activities (e.g., after cooking, the person may clean).\n",
    "5. Representation Fusion: the representations of the images (extracted from the Image LSTM) are combined with those of the sensors (from sensor embeddings). These combined representations are processed through Layer Normalization (to stabilize learning) and Dropout (to prevent overfitting).\n",
    "6. Output: Action Prediction: Finally, a Fully Connected (FC) Layer maps the final representation to a probability for each action. Another FC layer is used to predict the parameters associated with each action (such as the duration or intensity of the activity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c3378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomoticaAIDataset(Dataset):\n",
    "    def __init__(self, json_path, sensor_keys, context_len=3, transform=None):\n",
    "        with open(json_path, 'r') as f:\n",
    "            clusters = json.load(f)\n",
    "        self.context_len = context_len\n",
    "        self.transform = transform\n",
    "        self.sensor_keys = sensor_keys\n",
    "        self.num_sensor = 2 + len(sensor_keys)  # ora + weekday + custom sensori\n",
    "        self.records = []\n",
    "        for cluster in clusters:\n",
    "            imgs = [inp['img_path'] for inp in cluster['inputs']]\n",
    "            sensors = [inp.get('sensors', {}) for inp in cluster['inputs']]\n",
    "            actions = cluster.get('outputs', {}).get('actions', [])\n",
    "            for act in actions:\n",
    "                self.records.append({\n",
    "                    'imgs': imgs,\n",
    "                    'sensors': sensors,\n",
    "                    'action_name': act['action_name'],\n",
    "                    'params': act.get('params', [])\n",
    "                })\n",
    "            self.records.append({\n",
    "                'imgs': imgs,\n",
    "                'sensors': sensors,\n",
    "                'action_name': '<STOP>',\n",
    "                'params': []\n",
    "            })\n",
    "        self.a2i = self._build_action_vocab()\n",
    "        self.p2i = self._build_param_vocab()\n",
    "        history_tokens = set(self.a2i.keys()) | {'<NONE>'}\n",
    "        self.prev2i = {tok: idx for idx, tok in enumerate(sorted(history_tokens))}\n",
    "\n",
    "    def _build_action_vocab(self):\n",
    "        names = {r['action_name'] for r in self.records}\n",
    "        return {name: idx for idx, name in enumerate(sorted(names))}\n",
    "\n",
    "    def _build_param_vocab(self):\n",
    "        ps = set()\n",
    "        for r in self.records:\n",
    "            ps.update(r['params'])\n",
    "        return {name: idx for idx, name in enumerate(sorted(ps))}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.records[idx]\n",
    "        # immagini\n",
    "        imgs = []\n",
    "        for path in rec['imgs']:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "            img_t = self.transform(img) if self.transform else torch.from_numpy(np.array(img)).permute(2,0,1).float()\n",
    "            imgs.append(img_t)\n",
    "        # sensori\n",
    "        sensor_tensors = []\n",
    "        for sdict in rec['sensors']:\n",
    "            ts_str = sdict.get('timestamp') or sdict.get('file_timestamp')\n",
    "            if ts_str:\n",
    "                dt = datetime.fromisoformat(ts_str)\n",
    "                hour = dt.hour + dt.minute/60.0\n",
    "                weekday = dt.weekday()\n",
    "            else:\n",
    "                hour, weekday = 0.0, 0.0\n",
    "            vec = [hour, float(weekday)]\n",
    "            for key in self.sensor_keys:\n",
    "                val = sdict.get(key, 0)\n",
    "                try:\n",
    "                    vec.append(float(val))\n",
    "                except Exception:\n",
    "                    vec.append(float(hash(val) % 1000))\n",
    "            sensor_tensors.append(torch.tensor(vec, dtype=torch.float32))\n",
    "        # history azioni\n",
    "        start = max(0, idx - self.context_len)\n",
    "        hist = [r['action_name'] for r in self.records[start:idx]]\n",
    "        hist = ['<NONE>']*(self.context_len - len(hist)) + hist\n",
    "        hist_ids = [self.prev2i.get(h, self.prev2i['<NONE>']) for h in hist]\n",
    "        hist_tensor = torch.tensor(hist_ids, dtype=torch.long)\n",
    "        # target\n",
    "        act_id = list(self.a2i.values())[list(self.a2i.keys()).index(rec['action_name'])] if rec['action_name'] in self.a2i else 0\n",
    "        act_id = self.a2i[rec['action_name']]\n",
    "        param_vec = torch.zeros(len(self.p2i), dtype=torch.float32)\n",
    "        for p in rec['params']:\n",
    "            param_vec[self.p2i[p]] = 1.0\n",
    "        return {\n",
    "            'images': imgs,\n",
    "            'sensor_data': sensor_tensors,\n",
    "            'prev_actions': hist_tensor,\n",
    "            'action_id': torch.tensor(act_id, dtype=torch.long),\n",
    "            'param_vector': param_vec\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images_batch, sensor_batch = [], []\n",
    "    prev_actions_batch, action_ids_batch, param_vectors_batch = [], [], []\n",
    "    for sample in batch:\n",
    "        images_batch.append(sample['images'])\n",
    "        sensor_batch.append(sample['sensor_data'])\n",
    "        prev_actions_batch.append(sample['prev_actions'])\n",
    "        action_ids_batch.append(sample['action_id'])\n",
    "        param_vectors_batch.append(sample['param_vector'])\n",
    "    return {\n",
    "        'images': images_batch,\n",
    "        'sensor_data': sensor_batch,\n",
    "        'prev_actions': torch.stack(prev_actions_batch),\n",
    "        'action_ids': torch.stack(action_ids_batch),\n",
    "        'param_vectors': torch.stack(param_vectors_batch)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8444179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, List\n",
    "\n",
    "class DomoticaAI(nn.Module):\n",
    "    def __init__(self,\n",
    "                 person_detector,\n",
    "                 activity_classifier,\n",
    "                 num_sensor: int,\n",
    "                 num_prev_actions: int,\n",
    "                 num_output_actions: int,\n",
    "                 num_output_params: int,\n",
    "                 num_input_images: int = 3,\n",
    "                 embedding_dim: int = 128,\n",
    "                 prev_action_emb_dim: int = 64,\n",
    "                 action_emb_dim: int = 64,\n",
    "                 max_persons: int = 10,\n",
    "                 device: str = 'cpu',\n",
    "                 # >>> NEW: vocabolari opzionali <<<\n",
    "                 action_vocab: Optional[Dict[str, int]] = None,\n",
    "                 param_vocab: Optional[Dict[str, int]] = None,\n",
    "                 prev_vocab: Optional[Dict[str, int]] = None):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        self.num_sensor = num_sensor\n",
    "        self.num_prev_actions = num_prev_actions\n",
    "        self.num_output_actions = num_output_actions\n",
    "        self.num_output_params = num_output_params\n",
    "        self.num_input_images = num_input_images\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.prev_action_emb_dim = prev_action_emb_dim\n",
    "        self.action_emb_dim = action_emb_dim\n",
    "        self.max_persons = max_persons\n",
    "\n",
    "        self.person_detector = person_detector  # gi\u00e0 su device\n",
    "        self.activity_classifier = activity_classifier.to(self.device).eval()\n",
    "\n",
    "        self.activity_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(self.activity_classifier.num_classes, embedding_dim)\n",
    "            for _ in range(num_input_images)\n",
    "        ])\n",
    "        self.no_person_embeddings = nn.ParameterList([\n",
    "            nn.Parameter(torch.zeros(embedding_dim), requires_grad=True)\n",
    "            for _ in range(num_input_images)\n",
    "        ])\n",
    "        self.sensor_embeddings = nn.ModuleList([\n",
    "            nn.Linear(num_sensor, embedding_dim)\n",
    "            for _ in range(num_input_images)\n",
    "        ])\n",
    "\n",
    "        self.temporal_weights = nn.Parameter(torch.ones(num_input_images))\n",
    "        self.image_lstm = nn.LSTM(embedding_dim, embedding_dim, batch_first=True)\n",
    "        self.temporal_lstm = nn.LSTM(embedding_dim, embedding_dim, batch_first=True)\n",
    "\n",
    "        self.prev_action_embedding = nn.Embedding(num_prev_actions, prev_action_emb_dim)\n",
    "        self.prev_action_lstm = nn.LSTM(prev_action_emb_dim, 128, batch_first=True)\n",
    "\n",
    "        combined_dim = embedding_dim + 128\n",
    "        self.ln = nn.LayerNorm(combined_dim)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "        self.fc_act = nn.Linear(combined_dim, num_output_actions)\n",
    "        self.act_emb = nn.Embedding(num_output_actions, action_emb_dim)\n",
    "        self.fc_par = nn.Linear(combined_dim + action_emb_dim, num_output_params)\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "        # >>> NEW: allega i vocabolari e le inverse map <<<\n",
    "        # Se non forniti, crea mapping fittizi (utile solo per compatibilit\u00e0)\n",
    "        if action_vocab is None:\n",
    "            action_vocab = {str(i): i for i in range(num_output_actions)}\n",
    "        if param_vocab is None:\n",
    "            param_vocab = {str(i): i for i in range(num_output_params)}\n",
    "        if prev_vocab is None:\n",
    "            prev_vocab = {str(i): i for i in range(num_prev_actions)}\n",
    "        self._attach_vocabs(action_vocab, param_vocab, prev_vocab)\n",
    "\n",
    "    # --------------------\n",
    "    # Utility\n",
    "    # --------------------\n",
    "    def _attach_vocabs(self,\n",
    "                       action_vocab: Dict[str, int],\n",
    "                       param_vocab: Dict[str, int],\n",
    "                       prev_vocab: Dict[str, int]):\n",
    "        self.action_vocab: Dict[str, int] = dict(action_vocab)\n",
    "        self.param_vocab: Dict[str, int] = dict(param_vocab)\n",
    "        self.prev_vocab: Dict[str, int] = dict(prev_vocab)\n",
    "        # inverse map\n",
    "        self.i2a = {idx: name for name, idx in self.action_vocab.items()}\n",
    "        self.i2p = {idx: name for name, idx in self.param_vocab.items()}\n",
    "        self.i2prev = {idx: name for name, idx in self.prev_vocab.items()}\n",
    "        # controlli di consistenza (soft)\n",
    "        if len(self.action_vocab) != self.num_output_actions:\n",
    "            print(f\"[WARN] action_vocab size ({len(self.action_vocab)}) != num_output_actions ({self.num_output_actions})\")\n",
    "        if len(self.param_vocab) != self.num_output_params:\n",
    "            print(f\"[WARN] param_vocab size ({len(self.param_vocab)}) != num_output_params ({self.num_output_params})\")\n",
    "        if len(self.prev_vocab) != self.num_prev_actions:\n",
    "            print(f\"[WARN] prev_vocab size ({len(self.prev_vocab)}) != num_prev_actions ({self.num_prev_actions})\")\n",
    "\n",
    "    def crop(self, image: torch.Tensor, bbox: torch.Tensor):\n",
    "        to_pil = T.ToPILImage()\n",
    "        pil = to_pil(image)\n",
    "        x1, y1, x2, y2 = bbox.cpu().numpy().astype(int)\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        pad_x, pad_y = int(w*0.3), int(h*0.3)\n",
    "        x1f, y1f = max(0, x1-pad_x), max(0, y1-pad_y)\n",
    "        x2f = min(pil.width, x2+pad_x)\n",
    "        y2f = min(pil.height, y2+pad_y)\n",
    "        return pil.crop((x1f, y1f, x2f, y2f))\n",
    "\n",
    "    def process_single_image(self, image: torch.Tensor, sensor_vec: torch.Tensor, timestep: int):\n",
    "        det = self.person_detector.predict(image, score_threshold=0.5)\n",
    "        boxes, scores = det['boxes'], det['scores']\n",
    "        if len(boxes) > self.max_persons:\n",
    "            idx = torch.topk(scores, self.max_persons).indices\n",
    "            boxes = boxes[idx]\n",
    "\n",
    "        acts = []\n",
    "        for box in boxes:\n",
    "            sub = self.crop(image, box)\n",
    "            acts.append(self.activity_classifier.predict(sub))\n",
    "\n",
    "        if acts:\n",
    "            cls_t = torch.tensor(acts, device=self.device)\n",
    "            emb = self.activity_embeddings[timestep](cls_t)\n",
    "            if emb.dim() == 1:\n",
    "                emb = emb.unsqueeze(0)\n",
    "            emb_input = emb.unsqueeze(0)\n",
    "            _, (h_img, _) = self.image_lstm(emb_input)\n",
    "            img_repr = h_img[-1, 0, :]\n",
    "        else:\n",
    "            no_person_emb = self.no_person_embeddings[timestep]\n",
    "            no_person_input = no_person_emb.unsqueeze(0).unsqueeze(0)\n",
    "            _, (h_img, _) = self.image_lstm(no_person_input)\n",
    "            img_repr = h_img[-1, 0, :]\n",
    "\n",
    "        sensor_emb = self.sensor_embeddings[timestep](sensor_vec.to(self.device))\n",
    "        result = img_repr + sensor_emb\n",
    "        return result\n",
    "\n",
    "    def forward(self, images: List[List[torch.Tensor]], sensor_data: List[List[torch.Tensor]], prev_action_seq: torch.Tensor):\n",
    "        batch_size = len(images)\n",
    "        Tsteps = self.num_input_images\n",
    "        reps = []\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            step_reprs = []\n",
    "            for t in range(Tsteps):\n",
    "                img = images[i][t].to(self.device)\n",
    "                sensor_vec = sensor_data[i][t].to(self.device)\n",
    "                step_repr = self.process_single_image(img, sensor_vec, t)\n",
    "                step_reprs.append(step_repr)\n",
    "            seq = torch.stack(step_reprs, dim=0)\n",
    "            w = torch.softmax(self.temporal_weights, dim=0).view(-1, 1)\n",
    "            seq_w = (seq * w).unsqueeze(0)\n",
    "            _, (h_t, _) = self.temporal_lstm(seq_w)\n",
    "            reps.append(h_t.squeeze(0).squeeze(0))\n",
    "\n",
    "        img_r = torch.stack(reps, dim=0)\n",
    "\n",
    "        prev_emb = self.prev_action_embedding(prev_action_seq.to(self.device))\n",
    "        _, (h_p, _) = self.prev_action_lstm(prev_emb)\n",
    "        prev_r = h_p[-1]\n",
    "\n",
    "        combined = torch.cat([img_r, prev_r], dim=-1)\n",
    "        combined = self.ln(combined)\n",
    "        combined = self.drop(combined)\n",
    "\n",
    "        logits = self.fc_act(combined)\n",
    "        act_emb = torch.matmul(torch.softmax(logits, dim=-1), self.act_emb.weight)\n",
    "        params = self.fc_par(torch.cat([combined, act_emb], dim=-1))\n",
    "        return logits, params\n",
    "\n",
    "    def predict(self, images, sensor_data, prev_action_seq, threshold: float = 0.5):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            logits, params = self.forward(images, sensor_data, prev_action_seq)\n",
    "            action_id = logits.argmax(dim=-1).item()\n",
    "            action_name = self.i2a.get(action_id, str(action_id))\n",
    "            param_vec = params.squeeze(0)\n",
    "            # Dizionario {nome_param: score} filtrato per threshold\n",
    "            filtered = {self.i2p[i]: v.item() for i, v in enumerate(param_vec) if v.item() > threshold}\n",
    "            return action_id, action_name, filtered\n",
    "\n",
    "    # >>> NEW: helper per mappare id/score in etichette <<<\n",
    "    def idx_to_action(self, idx: int) -> str:\n",
    "        return self.i2a.get(idx, str(idx))\n",
    "\n",
    "    def vec_to_params(self, param_scores: torch.Tensor, threshold: float = 0.5) -> List[str]:\n",
    "        param_scores = param_scores.detach().cpu().flatten()\n",
    "        return [self.i2p[i] for i, v in enumerate(param_scores) if float(v) > threshold]\n",
    "\n",
    "    # --------------------\n",
    "    # Serializzazione singolo file\n",
    "    # --------------------\n",
    "    def save_singlefile(self, bundle_path: str):\n",
    "        cfg = {\n",
    "            'num_sensor': self.num_sensor,\n",
    "            'num_prev_actions': self.num_prev_actions,\n",
    "            'num_output_actions': self.num_output_actions,\n",
    "            'num_output_params': self.num_output_params,\n",
    "            'num_input_images': self.num_input_images,\n",
    "            'embedding_dim': self.embedding_dim,\n",
    "            'prev_action_emb_dim': self.prev_action_emb_dim,\n",
    "            'action_emb_dim': self.action_emb_dim,\n",
    "            'max_persons': self.max_persons,\n",
    "            'device': str(self.device),\n",
    "            'activity_num_classes': self.activity_classifier.num_classes,\n",
    "            'activity_arch': 'efficientnet_b0',\n",
    "            'format_version': 2,  # >>> NEW: bump versione\n",
    "        }\n",
    "\n",
    "        core_state = self.state_dict()\n",
    "        activity_state = self.activity_classifier.classifier.state_dict()\n",
    "\n",
    "        with open(self.person_detector._weights_path, 'rb') as f:\n",
    "            yolo_bytes = f.read()\n",
    "\n",
    "        package = {\n",
    "            'cfg': cfg,\n",
    "            'core_state': core_state,\n",
    "            'activity_state': activity_state,\n",
    "            'yolo_bytes': yolo_bytes,\n",
    "            # >>> NEW: includi i vocabolari <<<\n",
    "            'vocab': {\n",
    "                'action_vocab': self.action_vocab,\n",
    "                'param_vocab': self.param_vocab,\n",
    "                'prev_vocab': self.prev_vocab,\n",
    "            }\n",
    "        }\n",
    "\n",
    "        torch.save(package, bundle_path)\n",
    "        print(f\"[OK] Bundle salvato in: {bundle_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_singlefile(cls, bundle_path: str):\n",
    "        package = torch.load(bundle_path, map_location='cpu')  # poi spostiamo su device\n",
    "        cfg = package['cfg']\n",
    "\n",
    "        device = cfg.get('device', 'cpu')\n",
    "\n",
    "        # YOLO weights in tmp file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pt') as tmp:\n",
    "            tmp.write(package['yolo_bytes'])\n",
    "            tmp_path = tmp.name\n",
    "\n",
    "        try:\n",
    "            person_detector = YOLOPersonDetector(weights_path=tmp_path, device=device)\n",
    "        finally:\n",
    "            try:\n",
    "                os.remove(tmp_path)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        num_cls = cfg.get('activity_num_classes', 500)\n",
    "        activity_classifier = ActivityClassifier(num_classes=num_cls, device=device)\n",
    "        activity_classifier.classifier.load_state_dict(package['activity_state'])\n",
    "        activity_classifier.to(device)\n",
    "\n",
    "        # >>> NEW: recupera i vocabolari dal pacchetto (se presenti) <<<\n",
    "        vocabs = package.get('vocab', {})\n",
    "        action_vocab = vocabs.get('action_vocab')\n",
    "        param_vocab = vocabs.get('param_vocab')\n",
    "        prev_vocab = vocabs.get('prev_vocab')\n",
    "\n",
    "        # Modello core\n",
    "        model = cls(\n",
    "            person_detector=person_detector,\n",
    "            activity_classifier=activity_classifier,\n",
    "            num_sensor=cfg['num_sensor'],\n",
    "            num_prev_actions=cfg['num_prev_actions'],\n",
    "            num_output_actions=cfg['num_output_actions'],\n",
    "            num_output_params=cfg['num_output_params'],\n",
    "            num_input_images=cfg['num_input_images'],\n",
    "            embedding_dim=cfg['embedding_dim'],\n",
    "            prev_action_emb_dim=cfg['prev_action_emb_dim'],\n",
    "            action_emb_dim=cfg['action_emb_dim'],\n",
    "            max_persons=cfg['max_persons'],\n",
    "            device=device,\n",
    "            action_vocab=action_vocab,\n",
    "            param_vocab=param_vocab,\n",
    "            prev_vocab=prev_vocab,\n",
    "        )\n",
    "\n",
    "        model.load_state_dict(package['core_state'], strict=True)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(f\"[OK] Bundle caricato da: {bundle_path} su device {device}\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f86ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "CONTEXT_LEN = 3\n",
    "\n",
    "dataset = DomoticaAIDataset(\n",
    "    json_path='annotations.json',\n",
    "    sensor_keys=['luminosita_categorizzata_2'],\n",
    "    context_len=CONTEXT_LEN,\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "a2i    = dataset.a2i\n",
    "p2i    = dataset.p2i\n",
    "prev2i = dataset.prev2i\n",
    "num_sensor = dataset.num_sensor\n",
    "\n",
    "train_size = int(0.8 * len(dataset)) \n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6083f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_detector = YOLOPersonDetector(weights_path='yolo-domotica-ai.pt', device=device)\n",
    "activity_classifier = ActivityClassifier.load_from_file('activity_classifier_weights_best.pth', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DomoticaAI(\n",
    "    person_detector=person_detector,\n",
    "    activity_classifier=activity_classifier,\n",
    "    num_sensor=num_sensor,\n",
    "    num_prev_actions=len(prev2i),\n",
    "    num_output_actions=len(a2i),\n",
    "    num_output_params=len(p2i),\n",
    "    num_input_images=3,\n",
    "    embedding_dim=128,\n",
    "    prev_action_emb_dim=64,\n",
    "    action_emb_dim=64,\n",
    "    max_persons=10,\n",
    "    device=device,\n",
    "    action_vocab=a2i,   \n",
    "    param_vocab=p2i,     \n",
    "    prev_vocab=prev2i    \n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss) \n",
    "        loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "        \n",
    "class BinaryFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.register_buffer('alpha', None if alpha is None else torch.as_tensor(alpha, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        pt  = torch.exp(-bce)                  \n",
    "        loss = (1 - pt) ** self.gamma * bce\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            a = self.alpha\n",
    "            if a.dim() == 0:                 \n",
    "                a_pos = a\n",
    "                a_neg = 1 - a\n",
    "            else:                             \n",
    "                a_pos = a.view(1, -1)\n",
    "                a_neg = (1 - a).view(1, -1)\n",
    "            alpha_weight = targets * a_pos + (1 - targets) * a_neg\n",
    "            loss = loss * alpha_weight\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ac06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_criterion = nn.CrossEntropyLoss()\n",
    "action_criterion = FocalLoss(alpha=0.25, gamma=2)\n",
    "# param_criterion  = nn.BCEWithLogitsLoss()\n",
    "param_criterion = BinaryFocalLoss(alpha=0.25, gamma=2.0)\n",
    "optimizer        = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler        = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print(f\"Training on {device}, num_sensor={dataset.num_sensor}\")\n",
    "\n",
    "# Funzione per la validazione\n",
    "def validate(model, val_loader, action_criterion, param_criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_act_loss = 0.0\n",
    "    running_param_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(val_loader), desc=\"Validating\", total=len(val_loader))\n",
    "        for i, batch in pbar:  # <-- unpack (i, batch)\n",
    "            images       = batch['images']\n",
    "            sensor_data  = batch['sensor_data']\n",
    "            prev_actions = batch['prev_actions'].to(device)\n",
    "            action_ids   = batch['action_ids'].to(device)\n",
    "            param_vecs   = batch['param_vectors'].to(device)\n",
    "\n",
    "            logits, params = model(images, sensor_data, prev_actions)\n",
    "\n",
    "            # compute BOTH losses\n",
    "            loss_action = action_criterion(logits, action_ids)\n",
    "            loss_param  = param_criterion(params, param_vecs)\n",
    "\n",
    "            loss = loss_action + 0.5 * loss_param\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_act_loss += loss_action.item()\n",
    "            running_param_loss += loss_param.item()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'act': f\"{loss_action.item():.4f}\",\n",
    "                'par': f\"{loss_param.item():.4f}\",\n",
    "                'avg_loss': f\"{running_loss / (i+1):.4f}\",\n",
    "                'avg_act': f\"{running_act_loss / (i+1):.4f}\",\n",
    "                'avg_param': f\"{running_param_loss / (i+1):.4f}\",\n",
    "            })\n",
    "\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    avg_act_loss = running_act_loss / len(val_loader)\n",
    "    avg_param_loss = running_param_loss / len(val_loader)\n",
    "    print(f\"Validation avg loss: {avg_loss:.4f}, action loss: {avg_act_loss:.4f}, param loss: {avg_param_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# Ciclo di allenamento\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n=== Epoch {epoch+1}/{NUM_EPOCHS} ===\")\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_act_loss = 0.0\n",
    "    running_param_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), desc=\"Training\", total=len(train_loader))\n",
    "    for i, batch in pbar:\n",
    "        images      = batch['images']\n",
    "        sensor_data = batch['sensor_data']\n",
    "        prev_actions= batch['prev_actions'].to(device)\n",
    "        action_ids  = batch['action_ids'].to(device)\n",
    "        param_vecs  = batch['param_vectors'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, params = model(images, sensor_data, prev_actions)\n",
    "        loss_action = action_criterion(logits, action_ids)\n",
    "        loss_param  = param_criterion(params, param_vecs)\n",
    "        loss = loss_action + 0.5 * loss_param\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_act_loss += loss_action.item()\n",
    "        running_param_loss += loss_param.item()\n",
    "\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'act': f\"{loss_action.item():.4f}\", 'par': f\"{loss_param.item():.4f}\", \"avg_loss\": f\"{running_loss / (i+1):.4f}\", \"avg_act\": f\"{running_act_loss / (i+1):.4f}\", \"avg_param\": f\"{running_param_loss / (i+1):.4f}\"})\n",
    "    \n",
    "    # Average loss for training\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    avg_train_act_loss = running_act_loss / len(train_loader)\n",
    "    avg_train_param_loss = running_param_loss / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} training avg loss: {avg_train_loss:.4f}, action loss: {avg_train_act_loss:.4f}, param loss: {avg_train_param_loss:.4f}\")\n",
    "    \n",
    "    # Validation phase\n",
    "    avg_val_loss = validate(model, val_loader, action_criterion, param_criterion, device)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} avg train loss: {avg_train_loss:.4f}, avg validation loss: {avg_val_loss:.4f}, lr: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    # Salvataggio dei checkpoint\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch'               : epoch+1,\n",
    "            'model_state_dict'    : model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "        }, f'checkpoint_epoch_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03cd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_singlefile(bundle_path='model_definitive.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34efd137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "# --- 1) Caricamento modello dal bundle single-file ---\n",
    "def load_model(bundle_path, device=None):\n",
    "    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = DomoticaAI.load_singlefile(bundle_path)  # usa il tuo metodo\n",
    "    model.to(device).eval()\n",
    "    return model, device\n",
    "\n",
    "# --- 2) Preprocess di UNA immagine -> tensor CHW in [0,1] ---\n",
    "to_tensor = T.ToTensor()\n",
    "\n",
    "def load_one_image(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Immagine non trovata: {path}\")\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    # niente Normalize: YOLO nel tuo detector si aspetta 0..1\n",
    "    return to_tensor(img)  # Tensor CxHxW\n",
    "\n",
    "# --- helper: mappa una lista di nomi azione precedenti -> tensor di ID (B=1, seq_len) ---\n",
    "def prev_names_to_ids(prev_action_names, model, device, seq_len):\n",
    "    tok2id = getattr(model, \"prev_vocab\", None) or {}\n",
    "    none_id = tok2id.get(\"<NONE>\", 0)\n",
    "    ids = [tok2id.get(name, none_id) for name in (prev_action_names or [])]\n",
    "    # tieni solo gli ultimi seq_len elementi\n",
    "    ids = ids[-seq_len:]\n",
    "    # pad a sinistra con <NONE> per raggiungere seq_len\n",
    "    if len(ids) < seq_len:\n",
    "        ids = [none_id] * (seq_len - len(ids)) + ids\n",
    "    return torch.tensor([ids], dtype=torch.long, device=device)\n",
    "\n",
    "# --- 3) Costruzione input nel formato atteso dal tuo forward ---\n",
    "def make_inputs(image_paths, model, device, prev_action_names=None, prev_seq_len=8, sensor_fill=0.0):\n",
    "    num_steps = model.num_input_images          # dovrebbe essere 3\n",
    "    if len(image_paths) != num_steps:\n",
    "        raise ValueError(f\"Servono esattamente {num_steps} immagini, ricevute {len(image_paths)}\")\n",
    "\n",
    "    # images: List[ sample ] dove sample = List[ Tsteps ] di Tensor (C,H,W)\n",
    "    imgs_per_t = [load_one_image(p).to(device) for p in image_paths]\n",
    "    images = [imgs_per_t]  # batch size = 1\n",
    "\n",
    "    # sensor_data: stessa struttura di images ma con vettori (num_sensor,)\n",
    "    sensors_per_t = [\n",
    "        torch.full((model.num_sensor,), float(sensor_fill), device=device)\n",
    "        for _ in range(num_steps)\n",
    "    ]\n",
    "    sensor_data = [sensors_per_t]  # batch size = 1\n",
    "\n",
    "    # prev_action_seq da nomi -> id tramite il vocabolario caricato nel bundle\n",
    "    prev_action_seq = prev_names_to_ids(prev_action_names, model, device, prev_seq_len)\n",
    "\n",
    "    return images, sensor_data, prev_action_seq\n",
    "\n",
    "# --- 4) Inferenza semplice: usa predict() del tuo modello (con vocabs) ---\n",
    "def run_inference(bundle_path, image_paths, prev_action_names=None, threshold=0.5):\n",
    "    model, device = load_model(bundle_path)\n",
    "    images, sensor_data, prev_action_seq = make_inputs(\n",
    "        image_paths, model, device,\n",
    "        prev_action_names=prev_action_names,\n",
    "        prev_seq_len=8,\n",
    "        sensor_fill=0.0\n",
    "    )\n",
    "\n",
    "    # inferenza (nuova predict: (action_id, action_name, params_dict))\n",
    "    result = model.predict(images, sensor_data, prev_action_seq, threshold=threshold)\n",
    "\n",
    "    # compat: se hai la vecchia predict che ritorna solo (id, dict)\n",
    "    if isinstance(result, tuple) and len(result) == 3:\n",
    "        action_id, action_name, params = result\n",
    "    else:\n",
    "        action_id, params = result\n",
    "        # prova a decodificare il nome via vocabolario\n",
    "        action_name = getattr(model, \"idx_to_action\", lambda i: str(i))(action_id)\n",
    "\n",
    "    print(f\"Action ID: {action_id}\")\n",
    "    print(f\"Action Name: {action_name}\")\n",
    "\n",
    "    # ordina i parametri per score discendente e stampali belli\n",
    "    if isinstance(params, dict) and params:\n",
    "        sorted_params = sorted(params.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        print(f\"Params > {threshold}:\")\n",
    "        for k, v in sorted_params:\n",
    "            print(f\"  - {k}: {v:.3f}\")\n",
    "    else:\n",
    "        print(f\"Nessun parametro sopra {threshold}\")\n",
    "\n",
    "    return action_id, action_name, params\n",
    "\n",
    "# --------- USO ----------\n",
    "if __name__ == \"__main__\":\n",
    "    bundle_path = \"model_definitive.pth\"  # il tuo file salvato con save_singlefile\n",
    "    image_paths = [\n",
    "        \"/Users/eduardobolognini/Desktop/domotica ai/analizzatore immagini 2/cluster_triplet/cluster_0/group_1001/28-06_16-59-02.jpg\",\n",
    "        \"/Users/eduardobolognini/Desktop/domotica ai/analizzatore immagini 2/cluster_triplet/cluster_0/group_1001/28-06_17-00-03.jpg\",\n",
    "        \"/Users/eduardobolognini/Desktop/domotica ai/analizzatore immagini 2/cluster_triplet/cluster_0/group_1001/28-06_17-01-06.jpg\",\n",
    "    ]\n",
    "\n",
    "    # opzionale: storia azioni in chiaro (verranno mappate via prev_vocab del bundle)\n",
    "    prev_action_names = []\n",
    "\n",
    "    run_inference(bundle_path, image_paths, prev_action_names=prev_action_names, threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}